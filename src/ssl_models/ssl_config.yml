# ------------------------ Dataset & DataLoader Params ----------------------- #

sat_img_mean : [0.2132, 0.2890, 0.3737]
sat_img_std : [0.2092, 0.1928, 0.1706]
drn_img_mean : [0.4768, 0.5559, 0.4325]
drn_img_std : [0.1557, 0.1466, 0.1245]
milaid_img_mean : [0.3538, 0.3747, 0.3245]
milaid_img_std : [0.1941, 0.1764, 0.1750]
sat_hyp_img_mean : [1370.192,1184.382,1120.771,1136.260,1263.739,1645.403,1846.870,1762.595,1972.624,582.726,14.771,1732.164,1247.919]
sat_hyp_img_std : [633.152,650.284,965.231,948.982,1108.067,1258.364,1233.149,1364.387,3545.660,472.380,14.311,1310.370,1087.6702]

# ------------------------------ SimSiam Params ------------------------------ #

simsiam_params:
  "compute_collapse?" : True
  "base_lr" : 0.05
  "backbone" : resnet50
  #"proj_input_dim" : 2048 # harcoded as 2048 for resnet & 768 for swin-vit
  "proj_hidden_dim" : 2048
  "proj_output_dim" : 2048
  #"pred_input_dim" : 2048 #not needed, same as proj_output_dim
  "pred_hidden_dim" : 512
  "pred_output_dim" : 2048
  "weight_decay" : 0.0001

# -------------------------------- BYOL Params ------------------------------- #
byol_params:
  "base_lr" : 0.2 # We will be using this as the Warmup_start_lr
  "weight_decay" : 1.5e-6
  "scheduler_warmup_epochs" : 10
  "scheduler_eta_min" : 0
  #"proj_input_dim" : 2048 #harcoded as 2048 for resnet & 768 for swin-vit
  "proj_hidden_dim" : 4096
  "proj_output_dim" : 256
  #"pred_input_dim" : 256 # not needed, same as proj_output_dim
  "pred_hidden_dim" : 4096
  "pred_output_dim" : 256

# -------------------------------- Dino Params ------------------------------- #

dino_params:
  "base_lr" : 0.0005 # We will be using linear scaling rule 0.0005 * eff_bs /256
  "weight_decay" : 0.04
  "scheduler_warmup_epochs" : 10
  "scheduler_eta_min" : 0
  "proj_hidden_dim" : 2048
  "bottleneck_dim" : 256
  "proj_out_dim" : 4096 # mentioned as K in paper ; set as 65536 in experimentation, but 4096 also tested well in experimentation
  "freeze_proj_out_over_x_epochs" : 1 
  "local_view_size" : 96
  "global_view_size" : 224

# -------------------------------- MAE Params -------------------------------- #

mae_params:
  "decoder_dim" : 512
  "decoder_depth" : 8
  "decoder_heads" : 16 #! Check if this is default
  "mask_ratio" : 0.75
  "embed_dim" : 768
  #"apply_lr_scheduler?" : False
  "base_lr" : 1.5e-4 #1.5e-4 for pretraining, 1e-3 for finetuning
  "weight_decay" : 0.05
  "warmup_epochs" : 40 #40 for pretraining, 5 for finetuning
  "eta_min" : 0 #scheduler final decay value

# ---------------------------------- SatMAE ---------------------------------- #

satmae_params:
  "patch_size" : 8 #* This must be 8 due to pretrained weights
  "spatial_mask" : False # by default - False
  # For channel groups ammend script
  "channel_embed" : 256
  "embed_dim" : 768
  "depth" : 12
  "num_heads" : 12
  "decoder_channel_embed" : 128
  "decoder_embed_dim" : 512
  "decoder_depth" : 8
  "decoder_num_heads" : 16
  "mlp_ratio" : 4.
  "norm_pix_loss" : False

  "base_lr" : 0.001
  "weight_decay" : 0.05
  "warmup_epochs" : 40
  "eta_min" : 0
  "mask_ratio" : 0.75
  "pretrain_patch_hw" : 96

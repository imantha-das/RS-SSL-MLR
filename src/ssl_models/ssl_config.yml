# ------------------------ Dataset & DataLoader Params ----------------------- #

sat_img_mean : [0.2132, 0.2890, 0.3737]
sat_img_std : [0.2092, 0.1928, 0.1706]
drn_img_mean : [0.4768, 0.5559, 0.4325]
drn_img_std : [0.1557, 0.1466, 0.1245]

milaid_img_mean : [0.2132, 0.2890, 0.3737]
milaid_img_std : [0.2092, 0.1928, 0.1706]

dataloader_workers : 16

# ------------------------------ SimSiam Params ------------------------------ #

simsiam_params:
  "apply_lr_scheduler?" : True 
  "compute_collapse?" : True
  "base_lr" : 0.05
  "backbone" : resnet50
  #"proj_input_dim" : 2048 # harcoded as 2048 for resnet & 768 for swin-vit
  "proj_hidden_dim" : 2048
  "proj_output_dim" : 2048
  #"pred_input_dim" : 2048 #not needed, same as proj_output_dim
  "pred_hidden_dim" : 512
  "pred_output_dim" : 2048

# -------------------------------- BYOL Params ------------------------------- #
byol_params:
  "apply_lr_scheduler?" : True  
  "base_lr" : 0.2 # We will be using this as the Warmup_start_lr
  "weight_decay" : 1.5e-6
  "scheduler_warmup_epochs" : 10
  "scheduler_eta_min" : 0
  #"proj_input_dim" : 2048 #harcoded as 2048 for resnet & 768 for swin-vit
  "proj_hidden_dim" : 4096
  "proj_output_dim" : 256
  #"pred_input_dim" : 256 # not needed, same as proj_output_dim
  "pred_hidden_dim" : 4096
  "pred_output_dim" : 256

# -------------------------------- Dino Params ------------------------------- #

dino_params:
  "apply_lr_scheduler?" : True
  "base_lr" : 0.0005 # We will be using linear scaling rule 0.0005 * eff_bs /256
  "weight_decay" : 0.04
  "scheduler_warmup_epochs" : 10
  "scheduler_eta_min" : 0
  "proj_hidden_dim" : 2048
  "proj_out_dim" : 4096 # mentioned as K in paper ; set as 65536 in experimentation, but 4096 also tested well in experimentation
  "freeze_proj_out_over_x_epochs" : 1 
  "local_view_size" : 96
  "global_view_size" : 224

# -------------------------------- MAE Params -------------------------------- #

mae_params:
  "decoder_dim" : 512
  "decoder_depth" : 8
  "decoder_heads" : 16 #! Check if this is default
  "mask_ratio" : 0.75
  "embed_dim" : 768
  "apply_lr_scheduler?" : True
  "base_lr" : 1.5e-4 #1.5e-4 for pretraining, 1e-3 for finetuning
  "weight_decay" : 0.05
  "warmup_epochs" : 40 #40 for pretraining, 5 for finetuning
  "eta_min" : 0 #scheduler final decay value
